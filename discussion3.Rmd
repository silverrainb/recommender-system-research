---
output: html_document
---
<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}
</style>

<div class='alert alert-info'>
<h2 class='display-3 text-uppercase'>Recommender System Research - Youtube</h2>

#### Recommender System
##### CUNY MSDS DATA 643

### Rose Koh
##### 2018/06/26
<div class='clearfix'></div>
</div>

<div class='page-header text-uppercase'>
  <h4>Recommender System Research Discussion 3</h4>
</div>

<div class = 'well'>

Please complete the research discussion assignment in a Jupyter or R Markdown notebook. You should post the GitHub link to your research in a new discussion thread.

As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.

A few resources: 

When Recommendations Systems Go Bad; MLconf SEA 2016
Evan Estola https://www.youtube.com/watch?v=MqoRzNhrTnQ
http://cds.nyu.edu/recommendation-systems-go-bad-%E2%80%A8/


Equality of Opportunity in Supervised Learning
Moritz Hardt, Eric Price, Nathan Srebro
https://arxiv.org/pdf/1610.02413.pdf


Please make your post before our meetup on Thursday June 28, and respond to at least one other student's posts by our meetup on Tuesday July 3.

</div>

#### How does Youtube Recommender System works?

<div class="alert alert-success" role="alert">

A better recommendation system: prioritize ethics over profit
According to the source, many recommendation systems in media is in the state of reinforcing unethical targeting.  As the writer puts it, the recommendation engines are broken in ways that have grave consequences such as amplified conspiracy theories, gamified news, nonsense infiltrating mainstream discourse, misinformed voters. It has become the great polarizer.

A couple months ago, a video content creator showed up at YouTube’s offices with a gun last week, outraged that the platform had demonetized and downranked some of the videos on her channel. This, she felt, was censorship. It isn’t, but the Twitter conversation around the shooting clearly illustrated the simmering tensions over how platforms navigate content: there are those who hold an absolutist view on free speech and believe any moderation is censorship, and there are those who believe that moderation is necessary to facilitate norms that respect the experience of the community.

So how can we make the recommendation system more ethical?
For now, the recommender systems don't understand the content. It has just 1 goal: keep us clicking. That is the primary KPI chosen by companies. Both content-based or collaborative filtering recommender systems are not neutral as they rank and cluster what to show to people all the time.

A former YouTube recommendation engine architect has written extensively about the problem of YouTube serving up conspiratorial and radicalizing content—fiction outperforming reality. It has been on-going problems for years. Thanks to surveys, Wikipedia, and additional raters, the problems are less visible however it won’t impact the main problem—that YouTube’s algorithm is pushing users in a direction they might not want. 

Project Redirect, an effort by Google Jigsaw, redirects certain types of users who are searching YouTube for terrorist videos—people who appear to be motivated by more than mere curiosity. Rather than offer up more violent content, the approach of that recommendation system is to do the opposite—it points users to content intended to de-radicalize them. This project has been underway around violent extremism for a few years, which means that YouTube has been aware of the conceptual problem, and the amount of power their recommender systems wield, for some time now. It makes their decision to address the problem in other areas by redirecting users to Wikipedia for fact-checking even more baffling.

Giving people more control over what their algorithmic feed serves up is one potential solution. Twitter, for example, created a filter that enables users to avoid content from low-quality accounts. Not everyone uses it, but the option exists.


</div>

#### References
https://www.wired.com/story/creating-ethical-recommendation-engines/